{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Homework #4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjung2-oxy/Natural-Lang-Processing/blob/main/NLP_Homework_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nh-Tkz9No7_"
      },
      "source": [
        "**TFIDF Search Engine**:\n",
        "- To search for a user query, this code compares the average tfidf cosine similarity score of the query and the title, description, and transcript.\n",
        "- For search results that have close average tfidf cosine scores, we 'tiebreak' them by looking for matching named entities and nouns in the related topics and speakers categories. Lastly we give extra points to results that have highest number of views compared to other results with similar tfidf cosine scores.\n",
        "- To use, run all the functions before executing the last two blocks. The first of the last two will get the csv uploaded and do the preprocessing, this should take at most 5 minutes or so. The last cell asks for the search query and finds matching results to print. This will take about 30 seconds.\n",
        "- Improvements: with more time, we would've added extra points for more recently dated talks, tried bm25 instead of tfidf, looked at comments and duration for extra points, and made the searching run faster. As of now, each search is about 30 seconds because the looking for extra points for each results takes a bit of time. When a search query can't return a full 10 results, it prints out \"Other results you may be interested in\" by taking them from the related talks of the top result. This is not very advanced and so improving that would be awesome. What we were wishing we could've done is if a specific search didn't return anything, split the query apart and look at the words individually or do a 'did you mean...' search similar to how Google does it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVmWMKcKJbjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df6f6ec-b7fe-455e-f6f3-1cd0784c0014"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSYrIEOA4rZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e88fc85-b039-4586-8995-448c107d8125"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import ast\n",
        "from nltk import word_tokenize\n",
        "from gensim.summarization.bm25 import BM25\n",
        "from gensim import corpora\n",
        "import re\n",
        "from termcolor import colored\n",
        "from tqdm import tqdm as tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from pprint import pprint\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download(\"punkt\")\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rq--H35q1zuT"
      },
      "source": [
        "def preprocess(documents, remove_sw=True, stem=True, untokenized=True, verbose=True):\n",
        "    '''\n",
        "    Preprocesses documents by case folding, removing punctuation, tokenizing, stopword removal, stemming, and untokenizing before returning.\n",
        "\n",
        "    Inputs:\n",
        "    - documents : list of tuples of (text_item, ted_talk_id).\n",
        "    - remove_sw : toggles removal of stopwords\n",
        "    - stem : toggles stemming of tokens\n",
        "    - verbose: toggles progress bar for preprocessing (tqdm)\n",
        "\n",
        "    Output:\n",
        "    - processed_docs : list of tuples of (processed_text_item, ted_talk_id)\n",
        "\n",
        "    '''\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    processed_docs = []\n",
        "\n",
        "    # Preprocess documents\n",
        "    for doc, talk_id in tqdm(documents, total=len(documents), disable = not verbose):\n",
        "        # Case folding\n",
        "        doc = doc.lower();\n",
        "      \n",
        "        # Remove punctuation\n",
        "        import string\n",
        "        doc = \"\".join([char for char in doc if char not in string.punctuation])\n",
        "\n",
        "        # Tokenization for stopword removal and stemming\n",
        "        from nltk import word_tokenize\n",
        "        processed = word_tokenize(doc)\n",
        "\n",
        "        # Remove stopwords\n",
        "        if remove_sw == True:\n",
        "            stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "            processed = [word for word in processed if word not in stopwords]\n",
        " \n",
        "        # Stemming\n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        if stem == True:\n",
        "            porter = PorterStemmer()\n",
        "            processed = [porter.stem(word) for word in processed]\n",
        "\n",
        "        # Untokenize for TFIDF fitting.\n",
        "        if untokenized == True:\n",
        "            processed = ' '.join(word for word in processed)\n",
        "\n",
        "        # Append processed text as (text, talk_id)\n",
        "        processed_docs.append((processed, talk_id))\n",
        "\n",
        "    verboseprint(f\"Processed documents: {processed_docs}\")\n",
        "\n",
        "    return processed_docs\n",
        "\n",
        "\n",
        "def preprocess_no_id(documents, remove_sw=True, stem=True, untokenized=True, verbose=True):\n",
        "    '''\n",
        "    Preprocesses documents by case folding, removing punctuation, tokenizing, stopword removal, stemming, and untokenizing before returning.\n",
        "\n",
        "    Inputs:\n",
        "    - documents : list of documents\n",
        "    - remove_sw : toggles removal of stopwords\n",
        "    - stem : toggles stemming of tokens\n",
        "    - verbose: toggles progress bar for preprocessing (tqdm)\n",
        "\n",
        "    Output:\n",
        "    - processed_docs : list of processed documents\n",
        "\n",
        "    '''\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    processed_documents = []\n",
        "\n",
        "    if type(documents) == list:\n",
        "        # Preprocess documents\n",
        "        for doc in tqdm(documents, total=len(documents), disable = not verbose):\n",
        "            # Case folding\n",
        "            doc = doc.lower();\n",
        "          \n",
        "            # Remove punctuation\n",
        "            import string\n",
        "            doc = \"\".join([char for char in doc if char not in string.punctuation])\n",
        "\n",
        "            # Tokenization for stopword removal and stemming\n",
        "            from nltk import word_tokenize\n",
        "            processed = word_tokenize(doc)\n",
        "\n",
        "            # Remove stopwords\n",
        "            if remove_sw == True:\n",
        "                stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "                processed = [word for word in processed if word not in stopwords]\n",
        "    \n",
        "            # Stemming\n",
        "            from nltk.stem.porter import PorterStemmer\n",
        "            if stem == True:\n",
        "                porter = PorterStemmer()\n",
        "                processed = [porter.stem(word) for word in processed]\n",
        "\n",
        "            # Untokenize for TFIDF fitting.\n",
        "            if untokenized == True:\n",
        "                processed = ' '.join(word for word in processed)\n",
        "\n",
        "            # Append processed text as (text, talk_id)\n",
        "            processed_documents.append(processed)\n",
        "        verboseprint(f\"Processed Documents: {processed_documents}\")\n",
        "        return processed_documents\n",
        "\n",
        "    else:\n",
        "        doc = documents\n",
        "        # Case folding\n",
        "        doc = doc.lower();\n",
        "          \n",
        "        # Remove punctuation\n",
        "        import string\n",
        "        doc = \"\".join([char for char in doc if char not in string.punctuation])\n",
        "\n",
        "        # Tokenization for stopword removal and stemming\n",
        "        from nltk import word_tokenize\n",
        "        processed = word_tokenize(doc)\n",
        "\n",
        "        # Remove stopwords\n",
        "        if remove_sw == True:\n",
        "            stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "            processed = [word for word in processed if word not in stopwords]\n",
        "    \n",
        "        # Stemming\n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        if stem == True:\n",
        "            porter = PorterStemmer()\n",
        "            processed = [porter.stem(word) for word in processed]\n",
        "\n",
        "        # Untokenize for TFIDF fitting.\n",
        "        if untokenized == True:\n",
        "            processed = ' '.join(word for word in processed)\n",
        "\n",
        "        # Append processed text as (text, talk_id)\n",
        "        verboseprint(f\"Single doc: {processed}\")\n",
        "        return processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlK5sV73ytDy"
      },
      "source": [
        "def tfidf_fit(documents, top_n_terms=10):\n",
        "    '''\n",
        "    Creates a tfidf model and fits it with given text documents.\n",
        "\n",
        "    Inputs:\n",
        "    - documents : list of text documents\n",
        "    - top_n_terms : number of top terms to return\n",
        "\n",
        "    Outputs:\n",
        "    - tfidf_vect : TfIdfVectorizer model object\n",
        "    - df_tfidfvect : Dataframe of tfidf vectorized document tokens (index = tokens)\n",
        "    - tfidf_feature_names : list of top_n_terms with highest tfidf importance\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "    doc_matrix = tfidf_vect.fit_transform(documents)\n",
        "    data = doc_matrix.T.toarray()\n",
        "    tfidf_tokens = tfidf_vect.get_feature_names_out()\n",
        "    df_tfidfvect = pd.DataFrame(data, index=tfidf_tokens)\n",
        "\n",
        "    importance = np.argsort(np.asarray(doc_matrix.sum(axis=0)).ravel())[::-1]\n",
        "    tfidf_feature_names = np.array(tfidf_tokens)\n",
        "\n",
        "    return tfidf_vect, df_tfidfvect, tfidf_feature_names[importance[:top_n_terms]]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5nDNOML417E"
      },
      "source": [
        "def main_tfidf(ted_talks_df, verbose=False):\n",
        "    '''\n",
        "    Main driver function for preprocessing and tfidf vectorizing Ted Talk title, description, and transcript documents\n",
        "\n",
        "    Inputs:\n",
        "    - ted_talks_df : Pandas dataframe of ted_talks csv\n",
        "    - verbose : print if true, else do nothing (used with verboseprint)\n",
        "\n",
        "    Outputs (used along with get_top_matches so returns a lot of outputs):\n",
        "    - tfidf_title_model : tfidf vectorizer model object for Title documents\n",
        "    - tfidf_title_df : Pandas dataframe of TFIDF vectorized Title tokens (index = title tokens)\n",
        "    - processed_titles : list of lists with (preprocessed Title text, ted_talk_id)\n",
        "    - tfidf_description_model : tfidf vectorizer model object for Description documents\n",
        "    - tfidf_description_df : Pandas dataframe of TFIDF vectorized Description tokens (index = description tokens)\n",
        "    - processed_description : list of lists with (preprocessed description text, ted_talk_id)\n",
        "    - tfidf_transcript_model : tfidf vectorizer model object for Transcript documents\n",
        "    - tfidf_transcript_df : Pandas dataframe of TFIDF vectorized Description tokens (index = transcript tokens)\n",
        "    - processed_transcripts : list of lists with (preprocessed transcript text, ted_talk_id)\n",
        "\n",
        "    '''\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    # Data Preprocessing for Titles\n",
        "    title_docs = []\n",
        "    for index, row in ted_talks_df.iterrows():\n",
        "        title_docs.append((row[\"title\"], row[\"talk_id\"]))\n",
        "\n",
        "    print(\"Processing Title Documents\")\n",
        "    processed_titles = preprocess(title_docs, remove_sw=False, stem = True, verbose=verbose)\n",
        "    processed_title_docs, title_talk_ids = map(list, zip(*processed_titles))\n",
        "    verboseprint(f\"Length of processed title documents: {len(processed_title_docs)}\")\n",
        "    verboseprint(\"Example:\")\n",
        "    verboseprint(processed_title_docs[0])\n",
        "    verboseprint(\"Generating TfIdf for Titles\")\n",
        "    tfidf_title_model, tfidf_title_df, tfidf_title_topterms = tfidf_fit(processed_title_docs)\n",
        "    verboseprint(f\"Dataframe shape: {tfidf_title_df.shape}\")\n",
        "    verboseprint(f\"Top Terms (stopwords included): {tfidf_title_topterms}\")\n",
        "    verboseprint(\"-----------------------------------------------------\\n\")\n",
        "\n",
        "    # Data Preprocessing for Descriptions\n",
        "    description_docs = []\n",
        "    for index, row in ted_talks_df.iterrows():\n",
        "        description_docs.append((row[\"description\"], row[\"talk_id\"]))\n",
        "\n",
        "    print(\"Processing Description Documents\")\n",
        "    processed_description = preprocess(description_docs, remove_sw=True, stem = True, verbose=verbose)\n",
        "    processed_description_docs, description_talk_ids = map(list, zip(*processed_description))\n",
        "    verboseprint(f\"Length of processed description documents: {len(processed_description_docs)}\")\n",
        "    verboseprint(\"Example:\")\n",
        "    verboseprint(processed_description_docs[0])\n",
        "    verboseprint(\"Generating TfIdf for Descriptions\")\n",
        "    tfidf_description_model, tfidf_description_df, tfidf_description_topterms = tfidf_fit(processed_description_docs)\n",
        "    verboseprint(f\"Dataframe shape: {tfidf_description_df.shape}\")\n",
        "    verboseprint(f\"Top Terms (stopwords removed): {tfidf_description_topterms}\")\n",
        "    verboseprint(\"-----------------------------------------------------\\n\")\n",
        "\n",
        "    # Data Preprocessing for Transcripts\n",
        "    transcript_docs = []\n",
        "    for index, row in ted_talks_df.iterrows():\n",
        "        transcript_docs.append((row[\"transcript\"], row[\"talk_id\"]))\n",
        "\n",
        "    print(\"Processing Transcript Documents\")\n",
        "    processed_transcripts = preprocess(transcript_docs, verbose=verbose)\n",
        "    processed_transcript_docs, transcript_talk_ids = map(list, zip(*processed_transcripts))\n",
        "    verboseprint(f\"\\nLength of processed transcript documents: {len(processed_transcript_docs)}\")\n",
        "    verboseprint(\"Example:\")\n",
        "    verboseprint(processed_transcript_docs[0])\n",
        "    verboseprint(\"Generating TfIdf for Transcripts\")\n",
        "    tfidf_transcript_model, tfidf_transcript_df, tfidf_transcript_topterms = tfidf_fit(processed_transcript_docs)\n",
        "    verboseprint(f\"Dataframe shape: {tfidf_transcript_df.shape}\")\n",
        "    verboseprint(f\"Top Terms (stopwords removed): {tfidf_transcript_topterms}\")\n",
        "    verboseprint(\"-----------------------------------------------------\\n\")\n",
        "\n",
        "    return tfidf_title_model, tfidf_title_df, processed_titles, tfidf_description_model, tfidf_description_df, processed_description, tfidf_transcript_model, tfidf_transcript_df, processed_transcripts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd3WEphNFkbX"
      },
      "source": [
        "def process_query(query, tfidf_model, tfidf_df, title_search=False, verbose=False):\n",
        "    '''\n",
        "    Preprocesses query, and vectorizes with given tfidf model object\n",
        "\n",
        "    Inputs:\n",
        "    - query : user query (str)\n",
        "    - tfidf_model : tfidf model fitted with given documents (tfidf_model object)\n",
        "    - document_df : Pandas Dataframe of tfidf matrix for given documents\n",
        "    - title_search : whether to run preprocess with stopwords or not. For title search, do not remove stopwords.\n",
        "    - verbose : whether to print outputs (with verboseprint)\n",
        "\n",
        "    Outputs:\n",
        "    - q_veb : tfidf vectorized query\n",
        "\n",
        "    '''\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "    \n",
        "    verboseprint(\"Query:\", query)\n",
        "    if title_search == True:\n",
        "        processed_query = preprocess_no_id(user_query, remove_sw=False, stem=True, verbose=False)\n",
        "    else:\n",
        "        processed_query = preprocess_no_id(user_query, remove_sw=True, stem=True, verbose=False)\n",
        "    \n",
        "    verboseprint(\"\\nProcessed Query:\", processed_query + \"\\n\")\n",
        "\n",
        "    # Convert the query to a vector\n",
        "    q = [processed_query]\n",
        "    q_vec = tfidf_model.transform(q).toarray().reshape(tfidf_df.shape[0],)\n",
        "\n",
        "    return q_vec\n",
        "\n",
        "def get_similar_documents(query_vector, tfidf_model, df, docs, verbose=False):\n",
        "    '''\n",
        "    Find cosine similarity of query vector and every document in the given column.\n",
        "\n",
        "    Inputs:\n",
        "    - query_vector : tfidf vectorized user query\n",
        "    - tfidf_model : TfidfVectorizer() model object\n",
        "    - df : Pandas dataframe of tfidf matrix (index = document tokens)\n",
        "    - docs : list of tuples with (processed text, talk_id)\n",
        "    - verbose : whether to print outputs (with verboseprint)\n",
        "\n",
        "    Outputs:\n",
        "    - top_sorted : documents with cosine similarity scores >= 0.3 \n",
        "    - perfect_match : document with cosine similarity score of 1.0 (exact match)\n",
        "\n",
        "    '''\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    sim = {}\n",
        "    # Calculate the similarity\n",
        "    for i in range(len(docs)):\n",
        "      sim[i] = np.dot(df.loc[:, i].values, query_vector) / np.linalg.norm(df.loc[:, i]) * np.linalg.norm(query_vector)\n",
        "    \n",
        "    # Sort the values \n",
        "    sim_sorted = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # verboseprint the articles and their similarity values\n",
        "    perfect_match = 'NaN'\n",
        "    top_sorted = []\n",
        "    for k, v in sim_sorted:\n",
        "        if v >= 0.1:\n",
        "            top_sorted.append((docs[k][0], docs[k][1], v))\n",
        "\n",
        "        # Check for 'perfect' (exact) title match\n",
        "        if v == 1.0:\n",
        "            perfect_match = docs[k]\n",
        "    \n",
        "    return top_sorted, perfect_match\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJRlUCX4Khzx"
      },
      "source": [
        "def get_additional_matches(user_query, talk_id, ted_talks_df, complete_list, verbose=False):\n",
        "    '''\n",
        "    Count additional feature matches to tiebreak \n",
        "\n",
        "    Inputs:\n",
        "    - user_query : string of user inputted search query\n",
        "    - ted_talks_df : Pandas Dataframe of ted talks csv\n",
        "    - top_10_search_results : list of talk ids matching the search query \n",
        "    - verbose : whether to print or not (works with verboseprint and verbosepprint)\n",
        "\n",
        "    Outputs:\n",
        "    - top_10_search_results : completed search results with additional results\n",
        "\n",
        "    '''\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "    verbosepprint = pprint if verbose else lambda *a, **k: None\n",
        "\n",
        "    total_matches = 0\n",
        "\n",
        "\n",
        "    # POS tag user query\n",
        "    light_preprocess = preprocess_no_id(user_query, remove_sw=False, stem=False, untokenized=False, verbose=verbose)\n",
        "    preprocessed_query = light_preprocess\n",
        "    tagged = nltk.pos_tag(preprocessed_query)\n",
        "    verboseprint(tagged)\n",
        "\n",
        "    # Find any nouns in user query\n",
        "    query_nouns = []\n",
        "    for tag in tagged:\n",
        "        if tag[1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "            query_nouns.append(tag[0])\n",
        "\n",
        "    # Find named entities in search query\n",
        "    nlp = en_core_web_sm.load()\n",
        "    named_entities = nlp(user_query)\n",
        "    verboseprint(named_entities)\n",
        "\n",
        "    # Get row of current talk id\n",
        "    talk_row = ted_talks_df.loc[ted_talks_df['talk_id'] == talk_id]\n",
        "\n",
        "    all_speakers_txt = talk_row['all_speakers'].to_string(index=False)\n",
        "    all_speakers_dict = ast.literal_eval(all_speakers_txt.strip())\n",
        "    all_speakers = list(all_speakers_dict.values())\n",
        "    verboseprint(f\"All Speakers: {all_speakers}\")\n",
        "\n",
        "    topics_txt = talk_row['topics'].to_string(index=False)\n",
        "    topics_list = ast.literal_eval(topics_txt.strip())\n",
        "    verboseprint(f\"Related topics: {topics_list}\")\n",
        "\n",
        "    for entity in named_entities:\n",
        "        if str(entity) in all_speakers:\n",
        "            total_matches += 1\n",
        "\n",
        "    for noun in query_nouns:\n",
        "        if noun in topics_list:\n",
        "            total_matches += 1\n",
        "    \n",
        "    views = talk_row['views'].to_string(index=False)\n",
        "    views = int(views)        \n",
        "\n",
        "    return total_matches, views\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzWZ-ACG69nA"
      },
      "source": [
        "# For all talks within a specific range (default +-0.05), determine highest rank based on extra non-tfidf features\n",
        "\n",
        "def tiebreak(complete_list, user_query, ted_talks_df, range=0.05, verbose=False):\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    # sort smallest to largest\n",
        "    complete_list = sorted(complete_list, key= lambda x: x[1])\n",
        "    verboseprint(complete_list)\n",
        "\n",
        "    if len(complete_list) == 0:\n",
        "        return complete_list\n",
        "    elif len(complete_list) == 1:\n",
        "        \n",
        "        return complete_list\n",
        "    \n",
        "    \n",
        "    max_value = 0\n",
        "    min_value = complete_list[0][1]\n",
        "    for id, cosine_val in complete_list:\n",
        "        if cosine_val > max_value:\n",
        "            max_value = cosine_val\n",
        "        elif cosine_val < min_value:\n",
        "            min_value = cosine_val\n",
        "\n",
        "    verboseprint(f\"Max cosine: {max_value}\")\n",
        "    verboseprint(f\"Min cosine: {min_value}\")\n",
        "\n",
        "    bucketed_talks = []\n",
        "    temp_list = []\n",
        "    curr_ceiling = min_value + range\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    for id, avg_score in complete_list:\n",
        "        tiebreak_score, views = get_additional_matches(user_query, id, ted_talks_df, complete_list, verbose=False)\n",
        "        # verboseprint(f\"For {id}: Matching features is, {tiebreak_score}, and total views are, {views}\")\n",
        "\n",
        "        # Place last id into appropriate bucket\n",
        "        if i == (len(complete_list) - 1):\n",
        "            if avg_score <= curr_ceiling:\n",
        "                temp_list.append(list((id, avg_score, tiebreak_score, views)))\n",
        "            else:\n",
        "                bucketed_talks.append(temp_list)\n",
        "                temp_list = []\n",
        "                temp_list.append(list((id, avg_score, tiebreak_score, views)))\n",
        "            bucketed_talks.append(temp_list)\n",
        "            verboseprint(f\"Complete bucket, final id: {temp_list}\")\n",
        "\n",
        "        # Placing non-final ids\n",
        "        else:\n",
        "            if avg_score <= curr_ceiling:\n",
        "                temp_list.append(list((id, avg_score, tiebreak_score, views)))\n",
        "            else:\n",
        "                verboseprint(f\"Complete bucket: {temp_list}\")\n",
        "                bucketed_talks.append(temp_list)\n",
        "                temp_list = []\n",
        "                temp_list.append(list((id, avg_score, tiebreak_score, views)))\n",
        "                curr_ceiling = curr_ceiling + range\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    # Find talk with most views in bucket\n",
        "    for talks in bucketed_talks:\n",
        "        max_views = talks[0][3]\n",
        "        i = 0\n",
        "        for talk in talks:\n",
        "            views = talk[3]\n",
        "            if views >= max_views:\n",
        "                highest_viewed_talk = i\n",
        "            i += 1\n",
        "        # +1 to talk with most views in bucket\n",
        "        talks[highest_viewed_talk][2] = talks[highest_viewed_talk][2] + 1\n",
        "    \n",
        "\n",
        "    final_sorted_list = []\n",
        "    for talks in bucketed_talks:\n",
        "        verboseprint(f\"Bucket: {talks}\")\n",
        "        \n",
        "        sorted_talks = sorted(talks, key=lambda x: x[2] if x[2] > 0 else x[1], reverse=False)\n",
        "\n",
        "        for talk in sorted_talks:\n",
        "            final_sorted_list.append((talk[0], talk[1]))\n",
        "\n",
        "    final_sorted_list.reverse()\n",
        "    verboseprint(f\"Final list: {final_sorted_list}\")\n",
        "    \n",
        "    return final_sorted_list\n",
        "\n",
        "\n",
        "def get_average_cosine_score(titles_id_cosine, description_id_cosine, transcripts_id_cosine, top_titles, top_descriptions, top_transcripts, verbose=False):\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    complete_list = []\n",
        "    average_scores = {}\n",
        "\n",
        "     # Find average score between titles list, descriptions list, and \n",
        "    verboseprint(\"\\nFinding matches between top titles list and top descriptions list...\")\n",
        "    boosted_scores = []\n",
        "    for title, title_talk_id, title_cosine_val in top_titles:\n",
        "        for description, description_talk_id, description_cosine_val in top_descriptions:\n",
        "            if title_talk_id == description_talk_id:\n",
        "                boosted_cosine_val = title_cosine_val + description_cosine_val\n",
        "                boosted_scores.append((title_talk_id, boosted_cosine_val))\n",
        "    \n",
        "    sorted_boosted_scores = sorted(boosted_scores, key=lambda x: x[1], reverse=True)\n",
        "    title_desc_scores = titles_id_cosine\n",
        "    title_desc_scores.extend(description_id_cosine)\n",
        "    title_desc_scores.extend(sorted_boosted_scores)\n",
        "    sorted_title_desc_matches_scores = sorted(title_desc_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "    verboseprint(\"\\nCosine scores with added cosine scores for matching talk_ids between titles and descriptions.\")\n",
        "    if len(sorted_title_desc_matches_scores) > 0:\n",
        "        verboseprint(sorted_title_desc_matches_scores)\n",
        "    else:\n",
        "        verboseprint(\"None found.\")\n",
        "\n",
        "    boosted_scores = []\n",
        "    for talk_id, cosine_val in sorted_title_desc_matches_scores:\n",
        "        for transcript, transcript_talk_id, transcript_cosine_val in top_transcripts:\n",
        "            if talk_id == transcript_talk_id:\n",
        "                boosted_cosine_val = cosine_val + transcript_cosine_val\n",
        "                boosted_scores.append((talk_id, boosted_cosine_val))\n",
        "\n",
        "    complete_returns = sorted_title_desc_matches_scores\n",
        "    complete_returns.extend(transcripts_id_cosine)\n",
        "    complete_returns.extend(boosted_scores)\n",
        "    \n",
        "    averaged_complete_returns = []\n",
        "    for id, cosine_total in complete_returns:\n",
        "        cosine_average = cosine_total / 3\n",
        "        averaged_complete_returns.append((id, cosine_average))\n",
        "\n",
        "    sorted_avg_complete_returns = sorted(averaged_complete_returns, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_avg_complete_returns\n",
        "\n",
        "def get_top_matches(user_query, ted_talks_df, tfidf_title_model, tfidf_title_df, processed_titles, tfidf_description_model, tfidf_description_df, processed_description, tfidf_transcript_model, tfidf_transcript_df, processed_transcripts, verbose=False):\n",
        "    '''\n",
        "    Main driver for processing user query and finding cosine similarity of documents\n",
        "\n",
        "    Inputs( works with main_tfidf() ):\n",
        "    - user_query : user inputted query\n",
        "    - ted_talks_df : Pandas Dataframe of Ted Talks csv\n",
        "    - tfidf_title_model : tfidf vectorizer model object for Title documents\n",
        "    - tfidf_title_df : Pandas dataframe of TFIDF vectorized Title tokens (index = title tokens)\n",
        "    - processed_titles : list of lists with (preprocessed Title text, ted_talk_id)\n",
        "    - tfidf_description_model : tfidf vectorizer model object for Description documents\n",
        "    - tfidf_description_df : Pandas dataframe of TFIDF vectorized Description tokens (index = description tokens)\n",
        "    - processed_description : list of lists with (preprocessed description text, ted_talk_id)\n",
        "    - tfidf_transcript_model : tfidf vectorizer model object for Transcript documents\n",
        "    - tfidf_transcript_df : Pandas dataframe of TFIDF vectorized Description tokens (index = transcript tokens)\n",
        "    - processed_transcripts : list of lists with (preprocessed transcript text, ted_talk_id)\n",
        "    - verbose : whether to print or not (with verboseprint)\n",
        "\n",
        "    Outputs:\n",
        "    - top_10_search_results : list of 10 or less ted talk ids with the greatest cosine similarity scores (sorted highest to lowest)\n",
        "    - original_results_found : number of results found through tfidf before get_additional_matches() called\n",
        "\n",
        "    '''\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    top_10_search_results = []\n",
        "    best_title_matches = []\n",
        "    best_transcript_matches = []  \n",
        "    \n",
        "    verboseprint(\"\\nFINDING TOP TITLE MATCHES\")\n",
        "    query_vector = process_query(user_query, tfidf_model=tfidf_title_model, tfidf_df=tfidf_title_df, title_search=True)\n",
        "    top_titles, perfect_title_match = get_similar_documents(query_vector=query_vector, tfidf_model=tfidf_title_model, df=tfidf_title_df, docs=processed_titles, verbose=verbose)\n",
        "    verboseprint(f\"{len(top_titles)} matches found.\")\n",
        "    if len(top_titles) > 0:\n",
        "        verboseprint(f\"Example: {top_titles[0]}\")\n",
        "    titles_id_cosine = []\n",
        "    for title, talk_id, cosine_val in top_titles:\n",
        "        titles_id_cosine.append((talk_id, cosine_val))\n",
        "    if perfect_title_match != 'NaN':\n",
        "        verboseprint(f\"Perfect match found: {perfect_title_match}\")\n",
        "\n",
        "    verboseprint(\"\\nFINDING TOP DESCRIPTION MATCHES\")\n",
        "    query_vector = process_query(user_query, tfidf_model=tfidf_description_model, tfidf_df=tfidf_description_df, title_search=False)\n",
        "    top_descriptions, perfect_description_match = get_similar_documents(query_vector=query_vector, tfidf_model=tfidf_description_model, df=tfidf_description_df, docs=processed_description, verbose=verbose)\n",
        "    verboseprint(f\"{len(top_descriptions)} matches found.\")\n",
        "    if len(top_descriptions) > 0:\n",
        "        verboseprint(f\"Example: {top_descriptions[0]}\")\n",
        "    description_id_cosine = []\n",
        "    for description, talk_id, cosine_val in top_descriptions:\n",
        "        description_id_cosine.append((talk_id, cosine_val))\n",
        "    if perfect_description_match != 'NaN':\n",
        "        verboseprint(f\"Perfect match found: {perfect_description_match}\")\n",
        "\n",
        "    verboseprint(\"\\nFINDING TOP TRANSCRIPT MATCHES\")\n",
        "    query_vector = process_query(user_query, tfidf_model=tfidf_transcript_model, tfidf_df=tfidf_transcript_df)\n",
        "    top_transcripts, perfect_transcript_match = get_similar_documents(query_vector=query_vector, tfidf_model=tfidf_transcript_model, df=tfidf_transcript_df, docs=processed_transcripts, verbose=verbose)\n",
        "    verboseprint(f\"{len(top_transcripts)} matches found.\")\n",
        "    if len(top_transcripts) > 0:\n",
        "        verboseprint(f\"Example: {top_transcripts[0]}\")\n",
        "    transcripts_id_cosine = []\n",
        "    for transcript, talk_id, cosine_val in top_transcripts:\n",
        "        transcripts_id_cosine.append((talk_id, cosine_val))\n",
        "    if perfect_transcript_match != 'NaN':\n",
        "        verboseprint(f\"Perfect match found: {perfect_transcript_match}\")\n",
        "\n",
        "    # Add perfect match to top_10_search_results if found.\n",
        "    if perfect_title_match != 'NaN':\n",
        "        top_10_search_results.append(perfect_title_match[1])\n",
        "    elif perfect_description_match != 'NaN':\n",
        "        top_10_search_results.append(perfect_description_match[1])\n",
        "    elif perfect_transcript_match != 'NaN':\n",
        "        top_10_search_results.append(perfect_transcript_match[1])\n",
        "\n",
        "    sorted_averages = get_average_cosine_score(titles_id_cosine, description_id_cosine, transcripts_id_cosine, top_titles, top_descriptions, top_transcripts, verbose)\n",
        "    if len(sorted_averages) > 20:\n",
        "        sorted_averages = sorted_averages[0:20]\n",
        "\n",
        "    final_list = tiebreak(sorted_averages, user_query, ted_talks_df, range=0.05)\n",
        "\n",
        "\n",
        "    verboseprint(\"\\nCosine scores with added cosine scores for matching talk_ids.\")\n",
        "    if len(final_list) > 0:\n",
        "        verboseprint(final_list)\n",
        "    else:\n",
        "        verboseprint(\"None found.\")\n",
        "\n",
        "    # Add matches to top_10_search_results (a talk would only already be in the top_10_search results if it was a perfect match.)\n",
        "    for id, cosine, in final_list:\n",
        "        if id not in top_10_search_results:\n",
        "            top_10_search_results.append(id)\n",
        "\n",
        "    # If results greater than 10, only keep top 10.\n",
        "    num_results_found = len(top_10_search_results)\n",
        "    original_results_found = num_results_found\n",
        "    if num_results_found > 10:\n",
        "        top_10_search_results = top_10_search_results[0:10]\n",
        "        verboseprint(top_10_search_results)\n",
        "    \n",
        "    # If results equals 10, keep all 10.\n",
        "    elif num_results_found == 10:\n",
        "        verboseprint(top_10_search_results)\n",
        "    \n",
        "    # If results less than 10, look for more results.\n",
        "    elif 0 < num_results_found < 10:\n",
        "        top_result = top_10_search_results[0]\n",
        "        row = ted_talks_df.loc[ted_talks_df['talk_id'] == top_result]\n",
        "        related_talks = row[\"related_talks\"].to_string(index=False)\n",
        "\n",
        "        related_dict = ast.literal_eval(related_talks.strip())\n",
        "        related_talks_list = list(related_dict.keys())\n",
        "\n",
        "        for related_talk in related_talks_list:\n",
        "            if related_talk not in top_10_search_results and num_results_found < 10:\n",
        "                top_10_search_results.append(related_talk)\n",
        "                num_results_found += 1\n",
        "\n",
        "    return top_10_search_results, original_results_found"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_jcofGsGLXn"
      },
      "source": [
        "def get_search_query():\n",
        "      '''\n",
        "      Asks user for search query\n",
        "\n",
        "      Inputs:\n",
        "      N/A\n",
        "\n",
        "      Outputs:\n",
        "      - user_query : string of user inputted query.\n",
        "      \n",
        "      '''\n",
        "      user_query = input(\"Search:\")\n",
        "      return user_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZNsWi40-SAG"
      },
      "source": [
        "def print_search_results(top_10_search_results, original_results_found, ted_talks_df, verbose=True):\n",
        "    '''\n",
        "    Pretty print search results of user search query.\n",
        "\n",
        "    Inputs:\n",
        "    - top_10_search_results : list of highest cosine similarity score documents (length 10 or less)\n",
        "    - original_results_found : number of 'high quality' results found before get_additional_matches()\n",
        "    - ted_talks_df : Pandas Dataframe of Ted Talks csv\n",
        "    - verbose: whether to print or not (works with verboseprint)\n",
        "\n",
        "    Outputs:\n",
        "    N/A\n",
        "    '''\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    alternates = False\n",
        "    if len(top_10_search_results) == 0:\n",
        "        verboseprint(colored(f\"No results found. Please search for something else...\", 'red'))\n",
        "        true_results = 0\n",
        "    else:\n",
        "        verboseprint(colored(f\"{original_results_found} result(s) found. Top 10 returned:\", 'green'))\n",
        "\n",
        "    search_ranking = 1\n",
        "    for id in top_10_search_results[0:original_results_found]:\n",
        "        row = ted_talks_df.loc[ted_talks_df['talk_id'] == id]\n",
        "        title = row['title'].to_string(index=False)\n",
        "        main_speaker = row[\"speaker_1\"].to_string(index=False)\n",
        "        event = row[\"event\"].to_string(index=False)\n",
        "        description = row[\"description\"].to_string(index=False)\n",
        "        url = row['url'].to_string(index=False)\n",
        "        verboseprint(colored(f\"{search_ranking}: {title}\", 'blue', attrs=['bold']))\n",
        "        verboseprint(colored(\"    Presented by\" + main_speaker + \" at\" + event, 'blue'))\n",
        "        verboseprint(\"   \" + description)\n",
        "        verboseprint(colored(\"   \" + url, 'yellow'))\n",
        "        verboseprint(\"-----------------------------------------------\\n\")\n",
        "        search_ranking = search_ranking + 1\n",
        "\n",
        "    if len(top_10_search_results) - original_results_found > 0:\n",
        "        verboseprint(colored(f\"End of results.\", 'red'))\n",
        "        verboseprint(colored(f\"Based on top result, here are other talks you may be interested in...\", 'yellow'))\n",
        "        for id in top_10_search_results[original_results_found:]:\n",
        "            row = ted_talks_df.loc[ted_talks_df['talk_id'] == id]\n",
        "            title = row['title'].to_string(index=False)\n",
        "            main_speaker = row[\"speaker_1\"].to_string(index=False)\n",
        "            event = row[\"event\"].to_string(index=False)\n",
        "            description = row[\"description\"].to_string(index=False)\n",
        "            verboseprint(colored(f\"{search_ranking}: {title}\", 'blue', attrs=['bold']))\n",
        "            verboseprint(colored(\"    Presented by\" + main_speaker + \" at\" + event, 'blue'))\n",
        "            verboseprint(\"   \" + description)\n",
        "            verboseprint(\"-----------------------------------------------\\n\")\n",
        "            search_ranking = search_ranking + 1\n",
        "    \n",
        "    verboseprint(colored(f\"End of results.\", 'red'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYLPSra80bEg"
      },
      "source": [
        "# PRE SEARCHING FUNCTIONS (DO ONCE)\n",
        "filepath = '/content/drive/MyDrive/ted_talks_en.csv'\n",
        "ted_talks_df = pd.read_csv(filepath)\n",
        "tfidf_title_model, tfidf_title_df, processed_titles, tfidf_description_model, tfidf_description_df, processed_description, tfidf_transcript_model, tfidf_transcript_df, processed_transcripts = main_tfidf(ted_talks_df, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIXo9buCPC-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b326d05-5a20-4fcf-a7bf-f6c3712cbc0f"
      },
      "source": [
        "user_query = get_search_query()\n",
        "top_10_search_results, original_results_found = get_top_matches(user_query, ted_talks_df, tfidf_title_model, tfidf_title_df, processed_titles, tfidf_description_model, tfidf_description_df, processed_description, tfidf_transcript_model, tfidf_transcript_df, processed_transcripts, verbose=False)\n",
        "print_search_results(top_10_search_results, original_results_found, ted_talks_df, verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search:crack\n",
            "\u001b[32m8 result(s) found. Top 10 returned:\u001b[0m\n",
            "\u001b[1m\u001b[34m1:  What if cracks in concrete could fix themselves?\u001b[0m\n",
            "\u001b[34m    Presented by Congrui Jin at TED-Ed\u001b[0m\n",
            "    Concrete is the most widely used construction material in the world. It can be found in swathes of city pavements, bridges that span vast rivers and the tallest skyscrapers on earth. But it does have a weakness: it's prone to catastrophic cracking that has immense financial and environmental impact. What if we could avoid that problem? Congrui Jin explores how to create a more resilient concrete. [TED-Ed Animation by Aeon Production].\n",
            "\u001b[33m    https://www.ted.com/talks/congrui_jin_what_if_cracks_in_concrete_could_fix_themselves/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m2:  The freakonomics of crack dealing\u001b[0m\n",
            "\u001b[34m    Presented by Steven Levitt at TED2004\u001b[0m\n",
            "    \"Freakonomics\" author Steven Levitt presents new data on the finances of drug dealing. Contrary to popular myth, he says, being a street-corner crack dealer isn't lucrative: It pays below minimum wage. And your boss can kill you.\n",
            "\u001b[33m    https://www.ted.com/talks/steven_levitt_the_freakonomics_of_crack_dealing/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m3:  Cracking Stuxnet, a 21st-century cyber weapon\u001b[0m\n",
            "\u001b[34m    Presented by Ralph Langner at TED2011\u001b[0m\n",
            "    When first discovered in 2010, the Stuxnet computer worm posed a baffling puzzle. Beyond its sophistication loomed a more troubling mystery: its purpose. Ralph Langner and team helped crack the code that revealed this digital warhead's final target. In a fascinating look inside cyber-forensics, he explains how -- and makes a bold (and, it turns out, correct) guess at its shocking origins.\n",
            "\u001b[33m    https://www.ted.com/talks/ralph_langner_cracking_stuxnet_a_21st_century_cyber_weapon/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m4:  The mathematician who cracked Wall Street\u001b[0m\n",
            "\u001b[34m    Presented by Jim Simons at TED2015\u001b[0m\n",
            "    Jim Simons was a mathematician and cryptographer who realized: the complex math he used to break codes could help explain patterns in the world of finance. Billions later, he's working to support the next generation of math teachers and scholars. TED's Chris Anderson sits down with Simons to talk about his extraordinary life in numbers.\n",
            "\u001b[33m    https://www.ted.com/talks/jim_simons_the_mathematician_who_cracked_wall_street/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m5:  Why do your knuckles pop?\u001b[0m\n",
            "\u001b[34m    Presented by Eleanor Nelsen at TED-Ed\u001b[0m\n",
            "    Some people love the feeling of cracking their knuckles, while others cringe at the sound. But what causes that trademark pop? And is it dangerous? Eleanor Nelsen gives the facts behind joint popping. [Directed by Steve Belfer Creative, narrated by Addison Anderson]. \n",
            "\u001b[33m    https://www.ted.com/talks/eleanor_nelsen_why_do_your_knuckles_pop/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m6:  A plane you can drive\u001b[0m\n",
            "\u001b[34m    Presented by Anna Mracek Dietrich at TEDGlobal 2011\u001b[0m\n",
            "    A flying car -- it's an iconic image of the future. But after 100 years of flight and automotive engineering, no one has really cracked the problem. Pilot Anna Mracek Dietrich and her team flipped the question, asking: Why not build a plane that you can drive?\n",
            "\u001b[33m    https://www.ted.com/talks/anna_mracek_dietrich_a_plane_you_can_drive/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m7:  Why we laugh\u001b[0m\n",
            "\u001b[34m    Presented by Sophie Scott at TED2015\u001b[0m\n",
            "    Did you know that you're 30 times more likely to laugh if you're with somebody else than if you're alone? Cognitive neuroscientist Sophie Scott shares this and other surprising facts about laughter in this fast-paced, action-packed and, yes, hilarious dash through the science of cracking up.\n",
            "\u001b[33m    https://www.ted.com/talks/sophie_scott_why_we_laugh/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m8:  A \"self-healing\" asphalt\u001b[0m\n",
            "\u001b[34m    Presented by Erik Schlangen at TEDxDelft\u001b[0m\n",
            "    Paved roads are nice to look at, but they're easily damaged and costly to repair. Erik Schlangen demos a new type of porous asphalt made of simple materials with an astonishing feature: When cracked, it can be \"healed\" by induction heating. \n",
            "\u001b[33m    https://www.ted.com/talks/erik_schlangen_a_self_healing_asphalt/\u001b[0m\n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[31mEnd of results.\u001b[0m\n",
            "\u001b[33mOther talks you may be interested in...\u001b[0m\n",
            "\u001b[1m\u001b[34m9:  Are we running out of clean water?\u001b[0m\n",
            "\u001b[34m    Presented by Balsher Singh Sidhu at TED-Ed\u001b[0m\n",
            "    Despite water covering 71% of the planet's surface, more than half the world's population endures extreme water scarcity for at least one month a year. Current estimates predict that by 2040, up to 20 more countries could be experiencing water shortages. These statistics raise a startling question: is the Earth running out of clean water? Balsher Singh Sidhu takes a closer look at water consumption. [Directed by Kozmonot Animation Studio, narrated by Addison Anderson, music by Deniz Dogancay]. \n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[1m\u001b[34m10:  What's a smartphone made of?\u001b[0m\n",
            "\u001b[34m    Presented by Kim Preshoff at TED-Ed\u001b[0m\n",
            "    As of 2018, there are around 2.5 billion smartphone users in the world. If we broke open all the newest phones and split them into their component parts, that would produce around 85,000 kg of gold, 875,000 of silver, and 40,000,000 of copper. How did this precious cache get into our phones--and can we reclaim it? Kim Preshoff investigates the sustainability of phone production. [TED-Ed Animation by Compote Collective]. \n",
            "-----------------------------------------------\n",
            "\n",
            "\u001b[31mEnd of results.\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}