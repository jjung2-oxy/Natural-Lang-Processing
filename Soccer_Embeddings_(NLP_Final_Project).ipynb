{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soccer Embeddings (NLP Final Project).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjung2-oxy/Natural-Lang-Processing/blob/main/Soccer_Embeddings_(NLP_Final_Project).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl-P-GiEBWgB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj2KRkSo9GNc"
      },
      "source": [
        "!pip install statsbombpy\n",
        "!pip install umap-learn\n",
        "!pip install chart_studio\n",
        "import statsbombpy\n",
        "from statsbombpy import sb\n",
        "import pandas as pd\n",
        "from tqdm import tqdm as tqdm\n",
        "import os\n",
        "import json\n",
        "import ast \n",
        "import numpy as np\n",
        "import random\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models import KeyedVectors\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "import umap\n",
        "import chart_studio\n",
        "import chart_studio.plotly as py\n",
        "\n",
        "from tqdm._tqdm_notebook import tqdm_notebook\n",
        "tqdm_notebook.pandas()\n",
        "\n",
        "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujTxoufR-fQS"
      },
      "source": [
        "# LOADS ALL THE NECESSARY DATA FROM THE DATASET\n",
        "\n",
        "def load_events():\n",
        "    data_raw = []\n",
        "    directory = r'/content/drive/MyDrive/data/sample_events'\n",
        "    files = os.listdir(directory)\n",
        "\n",
        "    print('\\nLOADING ALL EVENTS')\n",
        "    for match in tqdm(files, total=len(files)):\n",
        "        with open(f'/content/drive/MyDrive/data/sample_events/{match}') as data_file:\n",
        "            data = json.load(data_file)\n",
        "            data_raw.append(pd.json_normalize(data, sep='_').assign(match_id=match))\n",
        "\n",
        "    all_events = pd.DataFrame()\n",
        "    all_events = all_events.append(data_raw, ignore_index=True)\n",
        "    return all_events\n",
        "\n",
        "def load_player_metadata():\n",
        "    data = []\n",
        "    dir = r'/content/drive/MyDrive/data/lineups'\n",
        "    files = os.listdir(dir)\n",
        "\n",
        "    print('\\nLoading Player Metadata')\n",
        "    for file_name in tqdm(files, total=len(files)):\n",
        "        with open(f'/content/drive/MyDrive/data/lineups/{file_name}') as data_file:\n",
        "            data_item = json.load(data_file)\n",
        "            home_line_up, away_line_up = data_item[0], data_item[1]\n",
        "            for player in home_line_up['lineup']:\n",
        "                data.append(pd.json_normalize(player, sep='_'))\n",
        "            for player in away_line_up['lineup']:\n",
        "                data.append(pd.json_normalize(player, sep='_'))\n",
        "\n",
        "    all_players_metadata = pd.DataFrame()\n",
        "    all_players_metadata = all_players_metadata.append(data, ignore_index=True)\n",
        "    return all_players_metadata\n",
        "\n",
        "def get_team_metadata():\n",
        "\n",
        "    data = []\n",
        "    dir = r'/content/drive/MyDrive/data/matches'\n",
        "    competitions_seasons = [name for name in os.listdir(dir)]\n",
        "    print('\\nLoading Team Metadata')\n",
        "\n",
        "    for season in tqdm(competitions_seasons, total=len(competitions_seasons)):\n",
        "        files = os.listdir(f'/content/drive/MyDrive/data/matches/{season}')\n",
        "        for file_name in files:\n",
        "            with open(f'/content/drive/MyDrive/data/matches/{season}/{file_name}') as data_file:\n",
        "                data_item = json.load(data_file)\n",
        "                for item in data_item:\n",
        "                    data.append(pd.json_normalize(item))\n",
        "\n",
        "    all_teams_metadata = pd.DataFrame()\n",
        "    all_teams_metadata = all_teams_metadata.append(data, ignore_index=True)\n",
        "\n",
        "    # Columns that apply to both home and away\n",
        "    neutral_cols = ['match_id', 'season.season_name', 'stadium.name', 'competition.competition_name', \n",
        "            'competition.country_name', 'competition_stage.name']\n",
        "\n",
        "    # Home Team Data\n",
        "    home_teams_metadata = all_teams_metadata[neutral_cols + ['home_team.home_team_name', 'home_team.country.name', 'home_team.home_team_gender']]\n",
        "    # Away Team Data\n",
        "    away_teams_metadata = all_teams_metadata[neutral_cols + ['away_team.away_team_name', 'away_team.country.name', 'away_team.away_team_gender']]\n",
        "    \n",
        "    # Rename shared columns\n",
        "    cols_mapper = {'season.season_name': 'season_name', 'competition.country_name': 'competition_country_name', 'competition_stage.name': 'stage_name', 'competition.competition_name': 'competition_name'}\n",
        "    home_teams_metadata.rename(columns=cols_mapper, inplace=True)\n",
        "    away_teams_metadata.rename(columns=cols_mapper, inplace=True)\n",
        "    # Rename home/away team columns seperately \n",
        "    home_teams_metadata.rename(columns={'home_team.home_team_name': 'team_name',\n",
        "                                        'home_team.home_team_gender': 'team_gender',\n",
        "                                        'home_team_managers': 'team_managers',\n",
        "                                        'home_team.country.name': 'country_name'}, inplace=True)\n",
        "    print(f\"Length of home_teams_metadata: {len(home_teams_metadata)}\")\n",
        "    away_teams_metadata.rename(columns={'away_team.away_team_name': 'team_name',\n",
        "                                        'away_team.away_team_gender': 'team_gender',\n",
        "                                        'away_team_managers': 'team_managers',\n",
        "                                        'away_team_country_name': 'country_name'}, inplace=True)\n",
        "    print(f\"Length of away_teams_metadata: {len(away_teams_metadata)}\")\n",
        "\n",
        "    # Append home and away metadata together (each row is a match metadata of one team)\n",
        "    teams_metadata = pd.DataFrame()\n",
        "    teams_metadata = teams_metadata.append(home_teams_metadata, ignore_index=True)\n",
        "    teams_metadata = teams_metadata.append(away_teams_metadata, ignore_index=True)\n",
        "    teams_metadata.drop('away_team.country.name', axis=1, inplace=True)\n",
        "    print(f\"Length of combined teams_metadata: {len(teams_metadata)}\")\n",
        "\n",
        "    return teams_metadata\n",
        "\n",
        "\n",
        "def load_matches_metadata() -> pd.DataFrame:\n",
        "    data = []\n",
        "\n",
        "    print('\\nLoading Matches Metadata')\n",
        "    dir = f'/content/drive/MyDrive/data/matches'\n",
        "    competitions_dirs = os.listdir(dir)\n",
        "    for season in tqdm(competitions_dirs, total=len(competitions_dirs)):\n",
        "        try:\n",
        "            files = os.listdir(f'/content/drive/MyDrive/data/matches/{season}')\n",
        "        except NotADirectoryError:\n",
        "            continue\n",
        "        for file_name in files:\n",
        "            with open(f'/content/drive/MyDrive/data/matches/{season}/{file_name}') as data_file:\n",
        "                data_item = json.load(data_file)\n",
        "                for item in data_item:\n",
        "                    data.append(pd.json_normalize(item, sep=\"_\"))\n",
        "\n",
        "    matches_metadata = pd.DataFrame()\n",
        "    matches_metadata = matches_metadata.append(data, ignore_index=True)\n",
        "    cols_mapper = {'season_season_name': 'season_name', 'competition_competition_name': 'competition_name'}\n",
        "    matches_metadata.rename(columns=cols_mapper, inplace=True)\n",
        "    matches_metadata = matches_metadata.drop_duplicates(subset=['match_id'])\n",
        "    \n",
        "    return matches_metadata\n",
        "\n",
        "all_events = load_events()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIiOiEfkClPN"
      },
      "source": [
        "# FUNCTIONS FOR ADDITIONAL ACTION ATTRIBUTES\n",
        "\n",
        "def get_centered_coordinates(data):\n",
        "\n",
        "    field_dimensions = (106.0, 68)\n",
        "\n",
        "    # Get all x coordinates and all y coordinates\n",
        "    x_columns = [c for c in data.columns if (c[-2:].lower() == 'x')]\n",
        "    y_columns = [c for c in data.columns if c[-2:].lower() == 'y']\n",
        "\n",
        "    # Convert yard to meters\n",
        "    for x_col in x_columns:\n",
        "        data[x_col] = (0.9144 * data[x_col]) - 0.5 * field_dimensions[0]\n",
        "    for y_col in y_columns:\n",
        "        data[y_col] = (-1 * (0.9144 * data[y_col]) - 0.5 * field_dimensions[1])\n",
        "\n",
        "    for col in [col for col in data.columns if 'location' in col]:\n",
        "        data[col] = data[col].apply(lambda val: ast.literal_eval(val) if isinstance(val, str) else val)\n",
        "        data[col] = data[col].apply(lambda val: ((0.9144 * val[0]) - 0.5 * field_dimensions[0]), (-1 * (0.9144 * val[1]) - 0.5 * field_dimensions[1]))\n",
        "\n",
        "    return data\n",
        "\n",
        "def yard_meter_convert(yards):\n",
        "    return 0.9144 * yards\n",
        "\n",
        "\n",
        "def get_centered_coordinates(data):\n",
        "    '''\n",
        "    Convert positions units to meters with origin at centre circle\n",
        "    '''\n",
        "\n",
        "    field_length = 106.0\n",
        "    field_width = 68\n",
        "\n",
        "    x_columns = [c for c in data.columns if (c[-2:].lower() == '_x')]\n",
        "    y_columns = [c for c in data.columns if c[-2:].lower() == '_y']\n",
        "    for x_col in x_columns:\n",
        "        data[x_col] = (yard_meter_convert(data[x_col]) - 0.5 * field_length)\n",
        "    for y_col in y_columns:\n",
        "        data[y_col] = -1 * (yard_meter_convert(data[y_col]) - 0.5 * field_width)\n",
        "\n",
        "    for col in [col for col in data.columns if 'location' in col]:\n",
        "        data[col] = data[col].apply(lambda string: ast.literal_eval(string) if isinstance(string, str) else string)\n",
        "        data[col] = data[col].apply(lambda val: (yard_meter_convert(val[0]) - 0.5 * field_length, -1 * (yard_meter_convert(val[1]) - 0.5 * field_width)) if isinstance(val, tuple) or isinstance(val, list) else val)\n",
        "    return data\n",
        "\n",
        "def check_if_shot_scored(action: pd.Series):\n",
        "    # Check if shot lead to goal\n",
        "\n",
        "    # Check if type_name is action\n",
        "    if 'type_name' in action:\n",
        "        #Check if action is a shot\n",
        "        if action['type_name'].lower() == 'shot':\n",
        "            outcome = action.get('shot_outcome_name', np.nan)\n",
        "\n",
        "            # Check if shot outcome is 'goal', if yes return true, else return false\n",
        "            if outcome is not np.nan and outcome.lower() == 'goal':\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "    return np.nan\n",
        "\n",
        "def check_if_shot_outside_box(action: pd.Series):\n",
        "\n",
        "    pitch_dimensions = (106.0, 68)\n",
        "  \n",
        "    # Return nan if not a shot\n",
        "    if action['type_name'] != 'Shot':\n",
        "        return np.nan\n",
        "\n",
        "    # Return nan if location is empty\n",
        "    if pd.isna(action['location']):\n",
        "        return np.nan\n",
        "\n",
        "    # Get x, y coordinates from location\n",
        "    if isinstance(action['location'], str):\n",
        "        x, y = ast.literal_eval(action['location'])\n",
        "    elif isinstance(action['location'], list) or isinstance(action['location'], tuple):\n",
        "        x, y = action['location']\n",
        "\n",
        "    # Inside box if y is within +- 16.5 from y center and distance from x to x end of pitch < 16.5 or if x distance from goal is more than 16.5 meters\n",
        "    if x > (pitch_dimensions[0] / 2) - 16.5 and abs(y) < 16.5:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def check_if_dribble_won(action: pd.Series):\n",
        "\n",
        "    # Check if action is dribble and whether dribble was succesful or unsuccesful\n",
        "    if 'type_name' in action:\n",
        "        if action['type_name'].lower() == 'dribble':\n",
        "            outcome = action.get('dribble_outcome_name', np.nan)\n",
        "            if outcome is not np.nan and outcome.lower() == 'complete':\n",
        "                return True\n",
        "            elif outcome is not np.nan and outcome.lower() == 'incomplete':\n",
        "                return False\n",
        "        \n",
        "    return np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeT4DfTv6Toa"
      },
      "source": [
        "# Gets the extra attributes for actions \n",
        "\n",
        "def get_action_attributes(all_events, verbose=False) -> pd.DataFrame:\n",
        "\n",
        "    verboseprint = print if verbose else lambda *a, **k: None\n",
        "\n",
        "    verboseprint('Adding attributes to actions...')\n",
        "\n",
        "    # Convert team names and player names to lower case\n",
        "    verboseprint(\"Converting team/player names to lowercase.\")\n",
        "    for col in ['team_name', 'player_name']:\n",
        "        all_events[col] = all_events[col].progress_apply(lambda name: name.lower() if isinstance(name, str) else name)\n",
        "\n",
        "    verboseprint('Adding centered location in meters.')\n",
        "    all_events = get_centered_coordinates(all_events)\n",
        "    verboseprint(\"X Coordinates:\")\n",
        "    all_events['POS_START_X'] = all_events['location'].progress_apply(\n",
        "        lambda val: val[0] if isinstance(val, tuple) or isinstance(val, list) else val)\n",
        "    verboseprint(\"Y Coordinates:\")\n",
        "    all_events['POS_START_Y'] = all_events['location'].progress_apply(\n",
        "        lambda val: val[1] if isinstance(val, tuple) or isinstance(val, list) else val)\n",
        "\n",
        "    verboseprint('Adding shot attributes.')\n",
        "    verboseprint('Ensure action is shot.')\n",
        "    all_events['is_shot'] = all_events.progress_apply(lambda action: 1 if action['type_name'] == 'Shot' else 0, axis=1)\n",
        "    verboseprint('Check if shot is goal.')\n",
        "    all_events['goal'] = all_events.progress_apply(lambda event: float(check_if_shot_scored(event)), axis=1)\n",
        "    verboseprint('Check if shot is out of box.')\n",
        "    all_events['out_of_box_shot'] = all_events.progress_apply(lambda event: float(check_if_shot_outside_box(event)), axis=1)\n",
        "    verboseprint('Check if shot is header.')\n",
        "    all_events['header_shot'] = all_events.progress_apply(lambda event: 1 if pd.notna(event['shot_body_part_name']) and event['shot_body_part_name'].lower() == 'head' else 0, axis=1)\n",
        "    verboseprint('Check if shot is penalty. ')\n",
        "    all_events['penalty_shot'] = all_events.progress_apply(lambda event: 1 if pd.notna(event['shot_type_name']) and event['shot_type_name'].lower() == 'penalty' else 0, axis=1)\n",
        "    verboseprint('Check if shot is free kick.')\n",
        "    all_events['free_kick_shot'] = all_events.progress_apply(lambda event: 1 if pd.notna(event['shot_type_name']) and event['shot_type_name'].lower() == 'free kick' else 0, axis=1)\n",
        "\n",
        "\n",
        "    verboseprint('Adding passing attributes.')\n",
        "    verboseprint('Get expected assists.')\n",
        "    all_events['xA'] = all_events['pass_assisted_shot_id'].progress_apply(lambda shot_id: all_events.loc[all_events['id'] == shot_id, 'shot_statsbomb_xg'].iloc[0] if isinstance(shot_id, str) else np.nan)\n",
        "    pass_recipient = 'pass_recipient_name'\n",
        "    verboseprint('Get recipient of pass.')\n",
        "    all_events[pass_recipient] = all_events[pass_recipient].progress_apply(lambda val: val.lower() if isinstance(val, str) else val)\n",
        "\n",
        "    verboseprint(\"Adding dribbling attributes.\")\n",
        "    verboseprint('Check if dribble succesful.')\n",
        "    all_events['dribble_won'] = all_events.progress_apply(lambda action: check_if_dribble_won(action), axis=1)\n",
        "\n",
        "    return all_events\n",
        "\n",
        "# Get the extra metadata from the dataset and add it to player Dataframe\n",
        "\n",
        "def get_enriched_players_metadata(events_data) -> dict:\n",
        "\n",
        "    players_metadata = load_player_metadata()\n",
        "\n",
        "    players_metadata['player_name_lower'] = players_metadata['player_name'].apply(lambda val: val.lower() if isinstance(val, str) else val)\n",
        "\n",
        "    vocab_data_cp = events_data[events_data['player_name'].notna()].copy()\n",
        "    players_2_positions = vocab_data_cp[\n",
        "        ['player_name', 'position_name', 'match_id']].copy() \\\n",
        "        .drop_duplicates(). \\\n",
        "        groupby(['player_name', 'position_name']). \\\n",
        "        agg({'match_id': np.size}).reset_index().sort_values(by='match_id', ascending=False)\n",
        "    players_2_positions = players_2_positions.drop_duplicates(subset=['player_name'], keep='first')\n",
        "    players_2_positions.set_index('player_name', inplace=True)\n",
        "    players_2_positions = players_2_positions.to_dict(orient='index')\n",
        "\n",
        "    players_2_jersey_num = players_metadata[['player_name', 'jersey_number']].copy(). \\\n",
        "        groupby(['player_name']). \\\n",
        "        agg({'jersey_number': np.size}).reset_index().sort_values(by='jersey_number', ascending=False)\n",
        "    players_2_jersey_num.set_index('player_name', inplace=True)\n",
        "    players_2_jersey_num = players_2_jersey_num.to_dict(orient='index')\n",
        "\n",
        "    matches_metadata = load_matches_metadata()\n",
        "    matches_metadata['match_id'] = matches_metadata['match_id'].astype(str)\n",
        "    matches_metadata = matches_metadata.set_index('match_id')\n",
        "    matches_metadata = matches_metadata.to_dict(orient='index')\n",
        "    vocab_data_cp['match_date'] = vocab_data_cp['match_id'].apply(\n",
        "        lambda match_: matches_metadata.get(str(match_).split('.')[0], {'match_date': 'unknown'})['match_date'])\n",
        "    players_2_teams = vocab_data_cp.sort_values(by=['match_date'], ascending=False)[\n",
        "        ['player_name', 'team_name']].copy()\n",
        "    players_2_teams = players_2_teams.drop_duplicates(subset=['player_name'], keep='first')\n",
        "    players_2_teams = players_2_teams.set_index('player_name')\n",
        "    players_2_teams = players_2_teams.to_dict(orient='index')\n",
        "\n",
        "    # Add most frequent position name into metadata > position_name\n",
        "    players_metadata['position_name'] = players_metadata['player_name'].apply( \\\n",
        "        lambda name_: players_2_positions.get(name_.lower(), {'position_name': np.nan})['position_name'])\n",
        "    players_metadata['jersey_number'] = players_metadata['player_name'].apply( \\\n",
        "        lambda name_: players_2_jersey_num.get(name_, {'jersey_number': np.nan})['jersey_number'])\n",
        "    players_metadata['team_name'] = players_metadata['player_name'].apply( \\\n",
        "        lambda name_: players_2_teams.get(name_.lower(), {'team_name': ''})['team_name'])\n",
        "    players_metadata = players_metadata.drop_duplicates(subset=['player_name'], keep='first')\n",
        "    players_metadata = players_metadata.set_index('player_name')\n",
        "    players_metadata = players_metadata.to_dict(orient='index')\n",
        "\n",
        "\n",
        "    return players_metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05q6pK85eUYG"
      },
      "source": [
        "# Run get_action_attributes and get_enriched_players_metadata functions to enrich action and player data.\n",
        "\n",
        "all_granular_events = get_action_attributes(all_events, verbose=True)\n",
        "enriched_player_metadata = get_enriched_players_metadata(all_granular_events)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZPEcaOA95b"
      },
      "source": [
        "# Take x,y location on pitch and convert to bin\n",
        "\n",
        "def get_location_on_pitch(x, y) -> (int, int):\n",
        "\n",
        "    pitch_dimensions= (106.0, 68)\n",
        "    num_x_bins = 6\n",
        "    num_y_bins = 4\n",
        "\n",
        "    bin_x_width, bin_y_width = np.ceil(pitch_dimensions[0] / num_x_bins), np.ceil(pitch_dimensions[1] / num_y_bins)\n",
        "    x, y = x + pitch_dimensions[0] / 2, y + pitch_dimensions[1] / 2\n",
        "\n",
        "    # Extract bin values [0, num bins - 1]\n",
        "    bin_x = int(min(np.floor(x / bin_x_width), num_x_bins - 1))\n",
        "    bin_y = int(min(np.floor(y / bin_y_width), num_y_bins - 1))\n",
        "\n",
        "    return f\"({str(bin_x + 1)}/{str(num_x_bins)}, {str(bin_y + 1)}/{str(num_y_bins)})\"\n",
        "\n",
        "# Tokenize action with action name and location\n",
        "def action_to_token(action) -> str:\n",
        "\n",
        "        # Add Action Name\n",
        "        action_name = action.get('type', action.get('type_name', np.nan))\n",
        "        \n",
        "        if action_name is np.nan:\n",
        "            return np.nan\n",
        "        else:\n",
        "            action_name = action_name.lower()\n",
        "\n",
        "        action_token = f'<{action_name}>'\n",
        "\n",
        "        # Add Action Location\n",
        "        if action['location'] is not np.nan:\n",
        "           \n",
        "            if isinstance(action['location'], list) or isinstance(action['location'], tuple):\n",
        "                # Location is list or tuple\n",
        "                x, y = action['location']\n",
        "            else:\n",
        "                # Location is string\n",
        "                x, y = ast.literal_eval(action['location'])\n",
        "\n",
        "            location = get_location_on_pitch(x, y)\n",
        "            action_token = action_token + f\"{location}\".replace(\" \", \"\")\n",
        "\n",
        "        return action_token.replace(\" \", \"_\")\n",
        "\n",
        "# Create corpus from actions.\n",
        "def actions_to_corpus(all_events) -> pd.DataFrame:\n",
        "\n",
        "        print(f\"vocabulary_data size: {all_events.shape}\\n\")\n",
        "        vocabulary_data = all_events.copy()\n",
        "\n",
        "        # Tokenize event data into strings (uses action_to_token())\n",
        "        print('\\nTokenizing Events...')\n",
        "        vocabulary_data['token'] = vocabulary_data.progress_apply(lambda action: action_to_token(action), axis=1)\n",
        "        all_events['token'] = vocabulary_data['token'].copy()\n",
        "        vocabulary_data = vocabulary_data[~vocabulary_data['token'].isna()]\n",
        "        print(f\"vocabulary_data size after processing and removing empty tokens: {all_events.shape}\\n\")\n",
        "\n",
        "        # Create unique vocabulary\n",
        "        vocabulary = [val for val in vocabulary_data['token'].unique() if val is not np.nan]\n",
        "        vocabulary.extend(['oov'])  \n",
        "        print(f'Length of vocabulary (excluding oov):', len(vocabulary) - 1)\n",
        "\n",
        "        # Create token_to_index and index_to_token\n",
        "        index_to_token = dict(enumerate(vocabulary))\n",
        "        index_to_token = {str(key): val for key, val in index_to_token.items()}\n",
        "        token_to_index = {val: key for key, val in index_to_token.items()}\n",
        "\n",
        "        # Set token index for each action\n",
        "        vocabulary_data['token_index'] = vocabulary_data['token'].apply(lambda token: token_to_index.get(token, token_to_index['oov']))\n",
        "        print(vocabulary_data.head(5))\n",
        "\n",
        "        # Remove non-relevant columns\n",
        "        vocabulary_data = vocabulary_data[['token_index', 'token'] + ['match_id', 'period', 'possession']]\n",
        "\n",
        "        for col in ['match_id', 'period', 'possession']:\n",
        "            vocabulary_data[col] = vocabulary_data[col].astype(str)\n",
        "        separator = '-'\n",
        "        vocabulary_data['sentence_key'] = vocabulary_data[['match_id', 'period', 'possession']].apply(lambda vec: separator.join(vec), axis=1)\n",
        "\n",
        "\n",
        "        print('\\nBuilding sentences...')\n",
        "      \n",
        "        # Sentences come from vocabulary_data grouped based on keys\n",
        "        sentences = vocabulary_data[['sentence_key', 'token_index']].groupby('sentence_key')\n",
        "\n",
        "        # Sentences are turned into lists, and we add numbered index\n",
        "        sentences = sentences['token_index'].agg(list).reset_index()\n",
        "        documents = sentences['sentence_key'].tolist()\n",
        "        sentences = sentences['token_index'].tolist()\n",
        "\n",
        "        # Merge sentences smaller than sampling window together\n",
        "        print('\\nConcatenating Documents to build sampling_window sized documents...')\n",
        "        sampling_window = 5\n",
        "        corpus = []\n",
        "        temp_actions_length = 0\n",
        "        temp_actions = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(sentence) >= sampling_window:\n",
        "                corpus.append(sentence[:])\n",
        "            else:\n",
        "                # Store too small sentence and move on\n",
        "                temp_actions.extend(sentence[:])\n",
        "                temp_actions_length += len(sentence)\n",
        "\n",
        "                # Once actions have built up to sufficient size, add as one sentence. Clear temp actions.\n",
        "                if temp_actions_length >= sampling_window:\n",
        "                    corpus.append(temp_actions[:])\n",
        "                    temp_actions_length = 0\n",
        "                    temp_actions = []\n",
        "\n",
        "        print('Final number of sentences:', len(corpus))\n",
        "\n",
        "        # Update vocabulary\n",
        "        corpus_flat = set([subitem for item in corpus for subitem in item if type(item) is list])\n",
        "        vocabulary_index = set([token_to_index[token] for token in vocabulary])\n",
        "\n",
        "        # Update vocabulary after merging sentences\n",
        "        vocabulary_index = corpus_flat.intersection(vocabulary_index)\n",
        "        vocabulary = [index_to_token[token_index] for token_index in vocabulary_index]\n",
        "        print('Final length of vocabulary:', len(vocabulary))\n",
        "\n",
        "        return vocabulary, vocabulary_index, sentences, corpus, all_events, token_to_index, index_to_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuygmhDPafNr"
      },
      "source": [
        "# Train word2vec model over action corpus\n",
        "def train_Word2Vec(corpus, vocabulary_index, index_to_token, verbose=False):\n",
        "    \n",
        "    print('Training Word2Vec model...')\n",
        "    w2vmodel = Word2Vec(corpus, min_count=10, size=32, window=5)\n",
        "    w2vmodel.train(corpus, total_examples=len(corpus), epochs=5, compute_loss=True)\n",
        "    \n",
        "    # Store just the words + their trained embeddings.\n",
        "    word_vectors = w2vmodel.wv\n",
        "\n",
        "    # Create W2V indexed vocabulary based only on chosen vocabulary\n",
        "    w2v_vocabulary = list(word_vectors.vocab.keys())\n",
        "    w2v_select_vocab_index = [token for token in w2v_vocabulary if token in vocabulary_index]\n",
        "    w2v_select_vocab = [index_to_token[token] for token in w2v_select_vocab_index]\n",
        "\n",
        "    return w2vmodel, word_vectors, w2v_select_vocab, w2v_select_vocab_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfcayVGqW1wr"
      },
      "source": [
        "# Cut down amount of action types in dataframe, keep only 'important' actions.\n",
        "action_types = ['Ball Receipt*', 'Pressure', 'Dispossessed', 'Duel', 'Ball Recovery', 'Dribbled Past', 'Dribble', 'Interception', 'Block', 'Foul Committed', 'Foul Won', 'Goal Keeper', 'Miscontrol', 'Clearance', 'Offside', 'Pass', 'Shot', 'Carry', 'Goal Keeper']\n",
        "chosen_granular_events = all_granular_events[all_granular_events['type_name'].isin(action_types)]\n",
        "chosen_granular_events.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73AWtutB-Xy4"
      },
      "source": [
        "# Run actions_to_corpus to convert action data to corpus\n",
        "\n",
        "vocabulary, vocabulary_index, sentences, events_corpus, all_events, token_to_index, index_to_token = actions_to_corpus(chosen_granular_events)\n",
        "all_events['token_index'] = all_events['token'].apply(lambda token: token_to_index.get(token, token_to_index['oov']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ0SCRX8Kag4"
      },
      "source": [
        "# Train word2vec model on action corpus\n",
        "\n",
        "w2vmodel, word_vectors, w2v_limited_vocab, w2v_limited_vocab_index = train_Word2Vec(events_corpus, vocabulary_index, index_to_token, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kimxeEl9cblz"
      },
      "source": [
        "# Get cosine similarity of actions individually.\n",
        "def get_similar_actions(word_vectors, w2v_limited_vocab_index, index_to_token, num_similar=5):\n",
        "\n",
        "    selected_tokens = random.sample(w2v_limited_vocab_index, num_similar)\n",
        "    for token_index in selected_tokens:\n",
        "        print(f'\\nToken index: {token_index}')\n",
        "        token = index_to_token[token_index]\n",
        "        print(f\"Token = {token}\")\n",
        "        similar = word_vectors.most_similar(token_index, topn=num_similar)  # get other similar words\n",
        "        print(f\" - {num_similar} most similar tokens: {[index_to_token[sim[0]] for sim in similar]}\")\n",
        "\n",
        "        similar_neg = word_vectors.most_similar(negative=[token_index])\n",
        "        print(f\" - {num_similar} most similar tokens to NEGATIVE form: \" f\"{[index_to_token[sim[0]] for sim in similar_neg]}\")\n",
        "\n",
        "        embedded_vocab = pd.DataFrame.from_dict({index_to_token[token_index]: word_vectors[token_index] for token_index in w2v_limited_vocab_index}, orient='index')\n",
        "\n",
        "    return embedded_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxSwshr4Ohhl"
      },
      "source": [
        "# Test validity of Word2Vec model by generating action comparisons\n",
        "\n",
        "embedded_vocab = get_similar_actions(word_vectors, w2v_limited_vocab_index, index_to_token, num_similar=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klxGM__-X0YZ"
      },
      "source": [
        "# Create corpus from player actions \n",
        "def players_to_corpus(all_events):\n",
        "    \n",
        "        print(f\"vocabulary_data size: {all_events.shape}\\n\")\n",
        "        vocabulary_data = all_events.copy()\n",
        "\n",
        "        # Tokenize event data into strings (uses action_to_token())\n",
        "        print('\\nTokenizing Events...')\n",
        "        vocabulary_data['token'] = vocabulary_data.progress_apply(lambda action: action_to_token(action), axis=1)\n",
        "        all_events['token'] = vocabulary_data['token'].copy()\n",
        "        vocabulary_data = vocabulary_data[~vocabulary_data['token'].isna()]\n",
        "        print(f\"vocabulary_data size after processing and removing empty tokens: {all_events.shape}\\n\")\n",
        "\n",
        "        # Create unique vocabulary\n",
        "        vocabulary = [val for val in vocabulary_data['token'].unique() if val is not np.nan]\n",
        "        vocabulary.extend(['oov'])  \n",
        "        print(f'Length of vocabulary (excluding oov):', len(vocabulary) - 1)\n",
        "\n",
        "        # Create token_to_index and index_to_token\n",
        "        index_to_token = dict(enumerate(vocabulary))\n",
        "        index_to_token = {str(key): val for key, val in index_to_token.items()}\n",
        "        token_to_index = {val: key for key, val in index_to_token.items()}\n",
        "\n",
        "        # Set token index for each action\n",
        "        vocabulary_data['token_index'] = vocabulary_data['token'].apply(lambda token: token_to_index.get(token, token_to_index['oov']))\n",
        "\n",
        "        # Remove non-relevant columns\n",
        "        vocabulary_data = vocabulary_data[['token_index', 'token'] + ['player_name', 'match_id']]\n",
        "\n",
        "        for col in ['player_name', 'match_id']:\n",
        "            vocabulary_data[col] = vocabulary_data[col].astype(str)\n",
        "        separator = '/'\n",
        "        vocabulary_data['sentence_key'] = vocabulary_data[['player_name', 'match_id']].apply(lambda vec: separator.join(vec), axis=1)\n",
        "\n",
        "\n",
        "        print('\\nBuilding sentences...')\n",
        "      \n",
        "        # Sentences come from vocabulary_data grouped based on keys\n",
        "        sentences = vocabulary_data[['sentence_key', 'token_index']].groupby('sentence_key')\n",
        "\n",
        "        # Sentences are turned into lists, and we add numbered index\n",
        "        sentences = sentences['token_index'].agg(list).reset_index()\n",
        "        documents = sentences['sentence_key'].tolist()\n",
        "        sentences = sentences['token_index'].tolist()\n",
        "\n",
        "        # add to corpus sentences that are longer than sampling window\n",
        "        sampling_window = 5\n",
        "        document_names = []\n",
        "        corpus= []\n",
        "        print('\\nBuilding Documents...')\n",
        "        for i, doc in tqdm(enumerate(sentences)):\n",
        "            if len(doc) >= sampling_window:\n",
        "                corpus.append(doc[:])\n",
        "                document_names.append(documents[i])\n",
        "        print('\\nFinal number of document_names:', len(document_names))\n",
        "\n",
        "        # Update vocabulary\n",
        "        corpus_flat = set([subitem for item in corpus for subitem in item if type(item) is list])\n",
        "        vocabulary_index = set([token_to_index[token] for token in vocabulary])\n",
        "\n",
        "        # Update vocabulary after merging sentences\n",
        "        vocabulary_index = corpus_flat.intersection(vocabulary_index)\n",
        "        vocabulary = [index_to_token[token_index] for token_index in vocabulary_index]\n",
        "        print('Final length of vocabulary:', len(vocabulary))\n",
        "\n",
        "        # Change names since code copied and slightly modified from action to corpus\n",
        "        player_vocabulary = vocabulary\n",
        "        player_vocabulary_index = vocabulary_index\n",
        "        player_corpus = corpus\n",
        "        player_token_to_index = token_to_index\n",
        "        player_index_to_token = index_to_token\n",
        "\n",
        "        return player_vocabulary, player_vocabulary_index, player_corpus, document_names, all_events, player_token_to_index, player_index_to_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl1GyWk0pti6"
      },
      "source": [
        "# Train doc2vec with player corpus\n",
        "def train_Doc2Vec(player_corpus) -> (Doc2Vec, KeyedVectors):\n",
        "    \n",
        "    # Train model\n",
        "    docs = player_corpus\n",
        "    docs_for_model = [TaggedDocument(doc, [i]) for i, doc in enumerate(docs)]\n",
        "\n",
        "    model = Doc2Vec(documents=docs_for_model, min_count=1, embedding_size=32, sampling_window=1)\n",
        "    model.train(docs_for_model, total_examples=len(docs_for_model), epochs=10)\n",
        "        \n",
        "    docs_embeddings = model.docvecs\n",
        "    \n",
        "    return model, docs_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkhQdYBZWgUO"
      },
      "source": [
        "# Train Player2Vec model and include averaging of player matches\n",
        "def player_w2v(all_events):\n",
        "\n",
        "    # Build corpus\n",
        "    print(f'\\nBuilding actions corpus and applying tokenization')\n",
        "    print(f'Building PlayerW2V corpus')\n",
        "    player_vocabulary, player_vocabulary_index, player_corpus, document_names, all_events, player_token_to_index, player_index_to_token = players_to_corpus(all_events)\n",
        "    print(document_names)\n",
        "    print(player_vocabulary)\n",
        "\n",
        "    \n",
        "    print(f\"Number of documents for model: {len(player_corpus)}\")\n",
        "    player_w2v_model, player_w2v_embeddings = train_Doc2Vec(player_corpus)\n",
        "\n",
        "    print(f\"Number of documents: {len(document_names)}\")\n",
        "    print(f\"Number of sentences in corpus: {len(player_corpus)}\")\n",
        "\n",
        "    # Build a comfortable dict for collecting players' vectors, later to be the players_embeddings\n",
        "    embedded_vocab = {}\n",
        "    for i, doc in enumerate(document_names):\n",
        "        embedded_vocab[doc] = player_w2v_model.infer_vector(player_corpus[i])\n",
        "\n",
        "    embedded_vocab = pd.DataFrame.from_dict(embedded_vocab, orient='index')\n",
        "    print(embedded_vocab.head(5))\n",
        "\n",
        "    # Build players embeddings - averages of their matches\n",
        "    \n",
        "    print(f'\\nBuilding Player embeddings')\n",
        "    separator = '/'\n",
        "    embedded_vocab = embedded_vocab.reset_index()\n",
        "    embedded_vocab['player_name'] = embedded_vocab['index'].apply(lambda index: index.split(separator)[0])\n",
        "    avg_player_embeddings = embedded_vocab.groupby('player_name').mean()\n",
        "\n",
        "    return player_w2v_model, all_events, player_corpus, embedded_vocab, avg_player_embeddings, player_w2v_embeddings, player_vocabulary_index, player_token_to_index, player_index_to_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1nKl-aUaOIX"
      },
      "source": [
        "# Generate player vectors\n",
        "\n",
        "player_w2v_model, all_events, player_corpus, embedded_vocab, avg_player_embeddings, player_w2v_embeddings, player_vocabulary_index, player_token_to_index, player_index_to_token = player_w2v(all_events)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xKtBpB0NqG"
      },
      "source": [
        "# Generate similarity matrix for all players\n",
        "\n",
        "def generate_doc_similarities(model_name: str, avg_player_embeddings):\n",
        "    print(f'\\nCalculating Player2Vec player_similarities')\n",
        "    player_similarities = {}\n",
        "    for i, (ix, doc_1) in tqdm(enumerate(avg_player_embeddings.iterrows()), total=avg_player_embeddings.shape[0]):\n",
        "        for j, doc_2 in avg_player_embeddings[i:].iterrows():\n",
        "            cosine_similarity = np.dot(doc_1, doc_2) / (np.linalg.norm(doc_1) * np.linalg.norm(doc_2))\n",
        "            player_similarities[(ix, j)] = {'cosine': cosine_similarity}\n",
        "    \n",
        "    return player_similarities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdQnwpvW6ju5"
      },
      "source": [
        "player_similarities = generate_doc_similarities('Player2Vec', avg_player_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50W97PQS6jAX"
      },
      "source": [
        "# Print most similar players to given player\n",
        "\n",
        "def get_similar_players(player_name, player_similarities):\n",
        "    \n",
        "    # (player 1 , player 2) : similarity\n",
        "    player_similarities = {(key[0].lower(), key[1].lower()): val for key, val in player_similarities.items()}\n",
        "\n",
        "    # Show most similar players by cosine similarity.\n",
        "            \n",
        "    curr_player_cosine_sim = {key[1]: val['cosine'] for key, val in player_similarities.items() if key[0] == player_name}\n",
        "    top_cosine_similarities = pd.Series(curr_player_cosine_sim).sort_values(ascending=False)\n",
        "    top_cosine_similarities = pd.Series(top_cosine_similarities[1:12].index, name='Player name')\n",
        "\n",
        "    return top_cosine_similarities\n",
        "\n",
        "def print_similar_players(player_name, top_cosine_similarities):\n",
        "    print(f\"Top 10 similar players to {player_name}: (By Cosine Similarity)\")\n",
        "    print(top_cosine_similarities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PssfI0QjhUX5"
      },
      "source": [
        "# Results for 'Suarez'\n",
        "top_cosine_similarities = get_similar_players('luis alberto suárez díaz', player_similarities)\n",
        "print_similar_players('luis alberto suárez díaz', top_cosine_similarities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4oGvQMdihM"
      },
      "source": [
        "# Reduces the player vectors to two dimensions to be plotted using the UMAP library.\n",
        "def get_reduced_representation(data):\n",
        "    \n",
        "    dimensions_reducer = umap.UMAP()\n",
        "    data = dimensions_reducer.fit_transform(data)\n",
        "    \n",
        "    return dimensions_reducer, data\n",
        "\n",
        "# Plot player embeddings using UMAP/Plotly\n",
        "def plot_embeddings(embedded_vocab, player_metadata):\n",
        "    doc_name_separator='/'\n",
        "\n",
        "    embedded_vocab_copy = embedded_vocab.copy()\n",
        "    dimensions_reducer, embedding = get_reduced_representation(embedded_vocab_copy)\n",
        "\n",
        "    for feature in [0, 1]:\n",
        "        embedded_vocab_copy[f'umap_{feature}'] = embedding[:, feature]\n",
        "\n",
        "\n",
        "    # Color the plotted players by position. Add team name into custom_data and name as label/hover_name\n",
        "    embedded_vocab_copy['color'] = [player_metadata.get(player.title(), {'position_name': ''})['position_name'] for player in embedded_vocab_copy.index.copy()]\n",
        "    embedded_vocab_copy['custom_data'] = [player_metadata.get(player.title(), {'team_name': ''})['team_name'] for player in embedded_vocab_copy.index.copy()]\n",
        "\n",
        "    embedded_vocab_copy['label'] = embedded_vocab_copy.index.copy()\n",
        "    embedded_vocab_copy['hover_name'] = embedded_vocab_copy.index.copy()\n",
        "\n",
        "\n",
        "    fig = px.scatter(embedded_vocab_copy, x='umap_0', y='umap_1', color='color',\n",
        "                     hover_name='hover_name', hover_data=[\"custom_data\", \"label\"],\n",
        "                     title='UMAP Projections of Players by Position')\n",
        "    fig.update_traces(textposition='top center')\n",
        "\n",
        "    fig.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW8vhzv8mlsm"
      },
      "source": [
        "plot_embeddings(avg_player_embeddings, player_metadata=enriched_player_metadata)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}